{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwAlsqm6kxPqCJvvro/UQM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StanleyLiangYork/2023_journal_club_CNN4N/blob/main/Neural_Network_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a demo for using numpy to implement a simple neural network.<p>\n",
        "It is based on the blog by Towards [Joe Sasson](https://towardsdatascience.com/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605)"
      ],
      "metadata": {
        "id": "r80a_3e95Wym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aKF_iz0q5JjB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "  \n",
        "    # we only implement the ReLu and Softmax activation in this demo\n",
        "    # Z - outputs of linear transformation of neurons Z = X*W+b\n",
        "    # A - outputs of the activation function A = activation(Z)\n",
        "    # W - weights of the neurons\n",
        "  \n",
        "    def __init__(self, neurons):\n",
        "        self.neurons = neurons\n",
        "        \n",
        "    def relu(self, inputs):\n",
        "        return np.maximum(0, inputs)\n",
        "\n",
        "    def softmax(self, inputs):\n",
        "        exp_scores = np.exp(inputs)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "        return probs\n",
        "    \n",
        "    def relu_derivative(self, dA, Z):\n",
        "      # the function to get the ReLu derivative/gradients of the ReLu activation\n",
        "        dZ = np.array(dA, copy = True)\n",
        "        dZ[Z <= 0] = 0\n",
        "        return dZ\n",
        "    \n",
        "    def forward(self, inputs, weights, bias, activation):\n",
        "      # the forward pass of the layer getting the layer outputs\n",
        "        Z_curr = np.dot(inputs, weights.T) + bias\n",
        "        \n",
        "        if activation == 'relu':\n",
        "            A_curr = self.relu(inputs=Z_curr)\n",
        "        elif activation == 'softmax':\n",
        "            A_curr = self.softmax(inputs=Z_curr)\n",
        "            \n",
        "        return A_curr, Z_curr\n",
        "    \n",
        "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n",
        "      # the backward pass / backpropagation of the layer getting the gradients\n",
        "        if activation == 'softmax':\n",
        "            dW = np.dot(A_prev.T, dA_curr)\n",
        "            db = np.sum(dA_curr, axis=0, keepdims=True)\n",
        "            dA = np.dot(dA_curr, W_curr) \n",
        "        else:\n",
        "            dZ = self.relu_derivative(dA_curr, Z_curr)\n",
        "            dW = np.dot(A_prev.T, dZ)\n",
        "            db = np.sum(dZ, axis=0, keepdims=True)\n",
        "            dA = np.dot(dZ, W_curr)\n",
        "            \n",
        "        return dA, dW, db"
      ],
      "metadata": {
        "id": "o1SvK3qU5xxk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.network = [] ## layers\n",
        "        self.architecture = [] ## mapping input neurons --> output neurons\n",
        "        self.params = [] ## W, b\n",
        "        self.memory = [] ## Z, A\n",
        "        self.gradients = [] ## dW, db\n",
        "        \n",
        "    def add(self, layer):\n",
        "        self.network.append(layer)\n",
        "            \n",
        "    def _compile(self, data):\n",
        "        for idx, layer in enumerate(self.network):\n",
        "            if idx == 0:\n",
        "              # the input layer\n",
        "                self.architecture.append({'input_dim':data.shape[1], 'output_dim':self.network[idx].neurons,\n",
        "                                         'activation':'relu'})\n",
        "            elif idx > 0 and idx < len(self.network)-1:\n",
        "              # the hidden layers\n",
        "                self.architecture.append({'input_dim':self.network[idx-1].neurons, 'output_dim':self.network[idx].neurons,\n",
        "                                         'activation':'relu'})\n",
        "            else:\n",
        "              # the output layers\n",
        "                self.architecture.append({'input_dim':self.network[idx-1].neurons, 'output_dim':self.network[idx].neurons,\n",
        "                                         'activation':'softmax'})\n",
        "        return self\n",
        "    \n",
        "    def _init_weights(self, data):\n",
        "      # initialize the weights of the neural network\n",
        "        self._compile(data)\n",
        "        \n",
        "        np.random.seed(99)\n",
        "        \n",
        "        for i in range(len(self.architecture)):\n",
        "            self.params.append({\n",
        "                'W':np.random.uniform(low=-1, high=1, \n",
        "                  size=(self.architecture[i]['output_dim'], \n",
        "                        self.architecture[i]['input_dim'])),\n",
        "                'b':np.zeros((1, self.architecture[i]['output_dim']))})\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def _forwardprop(self, data):\n",
        "      # forward pass of the data from input layer through the hidden layers and to the output layer\n",
        "        A_curr = data\n",
        "        \n",
        "        for i in range(len(self.params)):\n",
        "            A_prev = A_curr\n",
        "            A_curr, Z_curr = self.network[i].forward(inputs=A_prev, weights=self.params[i]['W'], \n",
        "                                           bias=self.params[i]['b'], activation=self.architecture[i]['activation'])\n",
        "            \n",
        "            self.memory.append({'inputs':A_prev, 'Z':Z_curr})\n",
        "            \n",
        "        return A_curr\n",
        "    \n",
        "    def _backprop(self, predicted, actual):\n",
        "      # backpropagation / backward pass of the network to get the gradients for network optimization\n",
        "        num_samples = len(actual)\n",
        "        \n",
        "        ## compute the gradient on predictions\n",
        "        dscores = predicted\n",
        "        dscores[range(num_samples),actual] -= 1\n",
        "        dscores /= num_samples\n",
        "        \n",
        "        dA_prev = dscores\n",
        "        \n",
        "        for idx, layer in reversed(list(enumerate(self.network))):\n",
        "            dA_curr = dA_prev\n",
        "            \n",
        "            A_prev = self.memory[idx]['inputs']\n",
        "            Z_curr = self.memory[idx]['Z']\n",
        "            W_curr = self.params[idx]['W']\n",
        "            \n",
        "            activation = self.architecture[idx]['activation']\n",
        "\n",
        "            dA_prev, dW_curr, db_curr = layer.backward(dA_curr, W_curr, Z_curr, A_prev, activation)\n",
        "\n",
        "            self.gradients.append({'dW':dW_curr, 'db':db_curr})\n",
        "            \n",
        "    def _update(self, lr=0.01):\n",
        "        for idx, layer in enumerate(self.network):\n",
        "            self.params[idx]['W'] -= lr * list(reversed(self.gradients))[idx]['dW'].T  \n",
        "            self.params[idx]['b'] -= lr * list(reversed(self.gradients))[idx]['db']\n",
        "    \n",
        "    def _get_accuracy(self, predicted, actual):\n",
        "        return np.mean(np.argmax(predicted, axis=1)==actual)\n",
        "    \n",
        "    def _calculate_loss(self, predicted, actual):\n",
        "        samples = len(actual)\n",
        "        \n",
        "        correct_logprobs = -np.log(predicted[range(samples),actual])\n",
        "        data_loss = np.sum(correct_logprobs)/samples\n",
        "\n",
        "        return data_loss\n",
        "    \n",
        "    def train(self, X_train, y_train, epochs):\n",
        "      # the train / optimization of the neural network\n",
        "        self.loss = []\n",
        "        self.accuracy = []\n",
        "        \n",
        "        self._init_weights(X_train)\n",
        "        \n",
        "        for i in range(epochs):\n",
        "            yhat = self._forwardprop(X_train)\n",
        "            self.accuracy.append(self._get_accuracy(predicted=yhat, actual=y_train))\n",
        "            self.loss.append(self._calculate_loss(predicted=yhat, actual=y_train))\n",
        "            \n",
        "            self._backprop(predicted=yhat, actual=y_train)\n",
        "            \n",
        "            self._update()\n",
        "            # print out the training information every 20 epochs\n",
        "            if i % 20 == 0:\n",
        "                s = 'EPOCH: {}, ACCURACY: {}, LOSS: {}'.format(i, self.accuracy[-1], self.loss[-1])\n",
        "                print(s)"
      ],
      "metadata": {
        "id": "rvEwH8_ATMY0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the following code, you need to register an account on [Kaggle](https://kaggle.com), after login, create a Kaggle credential file named 'kaggle.json' and upload it to colab. <p>\n",
        "click your account icon at the top right on your kaggle pack, select 'settings', go down to API, click \"Create New Token\". A new window will show up and allow you to download the 'kaggle.json' file to your local computer. <p>\n",
        "If you create a new token, the old one will expire, or you can mannually expire your old token and create a new one. <p>\n",
        "After finishing, you can use the kaggle tool to download the datasets from kaggle as I show you below."
      ],
      "metadata": {
        "id": "6A1yOOimj0_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assume you already uploaded the kaggle.json \n",
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "t_ytX1cIj7Uw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d uciml/iris; unzip iris.zip; rm iris.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFy9a6S4U0Md",
        "outputId": "d5987822-dcae-40b0-f2f0-9589f3b17049"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading iris.zip to /content\n",
            "\r  0% 0.00/3.60k [00:00<?, ?B/s]\n",
            "\r100% 3.60k/3.60k [00:00<00:00, 6.23MB/s]\n",
            "Archive:  iris.zip\n",
            "  inflating: Iris.csv                \n",
            "  inflating: database.sqlite         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(path):\n",
        "  data = pd.read_csv(path)\n",
        "  cols = list(data.columns)\n",
        "  feat_cols = cols[1:-1]\n",
        "  target = cols.pop()\n",
        "  X = data[feat_cols].copy()\n",
        "  y = data[target].copy()\n",
        "  y = LabelEncoder().fit_transform(y)\n",
        "  return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "Jc-6otMwV9nt"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/Iris.csv'\n",
        "X, y = get_data(path)"
      ],
      "metadata": {
        "id": "PzLJYhEJWZlM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmQMFqcmnYPt",
        "outputId": "07020a31-8706-4f21-dc7a-db9931043450"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 4) (150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network()\n",
        "model.add(DenseLayer(6)) # the first input dimension will check the data input dimension\n",
        "model.add(DenseLayer(8))\n",
        "model.add(DenseLayer(10))\n",
        "model.add(DenseLayer(3)) # the last / output layer use "
      ],
      "metadata": {
        "id": "AqneaXEUnJxe"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(X_train=X, y_train=y, epochs=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmDso6AMoE1Q",
        "outputId": "70094487-39a7-4e2a-fcd8-e4b014897d29"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0, ACCURACY: 0.3333333333333333, LOSS: 8.40744717002989\n",
            "EPOCH: 20, ACCURACY: 0.4, LOSS: 0.9215854842299206\n",
            "EPOCH: 40, ACCURACY: 0.43333333333333335, LOSS: 0.7536116948198673\n",
            "EPOCH: 60, ACCURACY: 0.42, LOSS: 0.6714779014672262\n",
            "EPOCH: 80, ACCURACY: 0.41333333333333333, LOSS: 0.6594143979448669\n",
            "EPOCH: 100, ACCURACY: 0.6666666666666666, LOSS: 0.5259943503852045\n",
            "EPOCH: 120, ACCURACY: 0.6666666666666666, LOSS: 0.4706373583820736\n",
            "EPOCH: 140, ACCURACY: 0.6666666666666666, LOSS: 0.5053203560733266\n",
            "EPOCH: 160, ACCURACY: 0.48, LOSS: 1.0150613941350863\n",
            "EPOCH: 180, ACCURACY: 0.8333333333333334, LOSS: 0.4606585529758215\n",
            "EPOCH: 200, ACCURACY: 0.96, LOSS: 0.14245395648443293\n",
            "EPOCH: 220, ACCURACY: 0.9733333333333334, LOSS: 0.11140924004633523\n",
            "EPOCH: 240, ACCURACY: 0.98, LOSS: 0.09284358413813795\n",
            "EPOCH: 260, ACCURACY: 0.98, LOSS: 0.08381489750531364\n",
            "EPOCH: 280, ACCURACY: 0.98, LOSS: 0.08094377825736056\n"
          ]
        }
      ]
    }
  ]
}